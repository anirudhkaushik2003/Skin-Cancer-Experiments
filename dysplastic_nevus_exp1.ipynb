{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import tqdm\n",
    "\n",
    "# Read image\n",
    "files  = sorted(glob.glob(\"/ssd_scratch/cvit/anirudhkaushik/dataset/ISIC_2019/ISIC_2019_Training_Input/*.jpg\"))\n",
    "print(len(files))\n",
    "# for f in tqdm.tqdm(files):\n",
    "#     img = cv2.imread(f, cv2.IMREAD_COLOR)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     # resize to 224x224\n",
    "#     img = cv2.resize(img, (224, 224),cv2.INTER_AREA )\n",
    "#     # save over original\n",
    "#     cv2.imwrite(f, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.models import ResNet101_Weights, resnet101\n",
    "\n",
    "# Load pretrained model\n",
    "# model = resnet101(weights=ResNet101_Weights.IMAGENET1K_V2)\n",
    "model = resnet101()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"/ssd_scratch/cvit/anirudhkaushik/dataset/ISIC_2019/ISIC_2019_Training_GroundTruth.csv\")\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "def load_isic_2019():\n",
    "    root = \"/ssd_scratch/cvit/anirudhkaushik/dataset/ISIC_2019\"\n",
    "    \"\"\"\n",
    "    Load ISIC_2019 dataset and convert it to IIRC format\n",
    "\n",
    "    Args:\n",
    "        root (string): The location of the dataset\n",
    "        intask_valid_train_ratio (float): the percentage of the training set to be taken for the in-task validation set\n",
    "            , a training-like validation set used for valdation during the task training (default: 0.1)\n",
    "        posttask_valid_train_ratio (float): the percentage of the training set to be taken for the post-task validation\n",
    "            set, a test-like validation set used for valdation after the task training (default: 0.1)\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, DatasetStructType]: datasets, a dictionary with the keys corresponding to the four splits (train,\n",
    "        intask_validation, posttask_validation, test), and the values being a list of the samples that belong to\n",
    "        each split (with the images provided in Image.Image type) in the DatasetTypeStruct structure\n",
    "    \"\"\"\n",
    "    raw_data_meta_df = pd.read_csv(root+'/ISIC_2019_Training_GroundTruth.csv')\n",
    "\n",
    "    isic_data_map = {\n",
    "        \"MEL\": \"Melanoma\",  \n",
    "        \"NV\": \"Melanocytic_nevus\" ,\n",
    "        \"BCC\": \"Basal_cell_carcinoma\",\n",
    "        \"AK\": \"Actinic_keratosis\",\n",
    "        \"BKL\": \"Benign_keratosis\",\n",
    "        \"DF\": \"Dermatofibroma\",\n",
    "        \"VASC\": \"Vascular_lesion\",\n",
    "        \"SCC\": \"Squamous_cell_carcinoma\"\n",
    "    }\n",
    "    \n",
    "    labels = list(raw_data_meta_df.columns[1:-1])\n",
    "    class_to_idx = {isic_data_map[label]: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    num_samples_nevus = 0\n",
    "    num_samples_mel = 0\n",
    "\n",
    "    X = raw_data_meta_df.iloc[:]['image'] # only image names, not actual images\n",
    "    y = raw_data_meta_df.iloc[:, 1:]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1, stratify=y)\n",
    "\n",
    "    raw_data_train = []\n",
    "    for ind  in range(len(X_train)):\n",
    "        img_name = X_train.iloc[ind]\n",
    "        labels = y_train.iloc[ind]\n",
    "        label = labels[labels == 1].index[0]\n",
    "        if label != \"MEL\" and label != \"NV\" : # only take MEL and NV\n",
    "            continue\n",
    "        # if label == \"NV\":\n",
    "        #     if num_samples_nevus >= 4070:\n",
    "        #         continue\n",
    "        #     num_samples_nevus += 1\n",
    "        # if label == \"MEL\":\n",
    "        #     num_samples_mel += 1\n",
    "        image = cv2.imread(os.path.join(root, \"ISIC_2019_Training_Input\", img_name+\".jpg\"), cv2.IMREAD_COLOR)\n",
    "        image = cv2.resize(image, (224, 224), cv2.INTER_AREA) # remove later\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "        label = class_to_idx[isic_data_map[label]] \n",
    "        raw_data_train.append((image, label))\n",
    "\n",
    "    print(f\"Train set, Number of nevus samples: {num_samples_nevus}\\n Number of melanoma samples: {num_samples_mel}\")\n",
    "    num_samples_mel = 0\n",
    "    num_samples_nevus = 0\n",
    "\n",
    "    raw_data_test = []\n",
    "    for ind  in range(len(X_test)):\n",
    "        img_name = X_test.iloc[ind]\n",
    "        labels = y_test.iloc[ind]\n",
    "        label = labels[labels == 1].index[0]\n",
    "        if label != \"MEL\" and label != \"NV\" : # only take MEL and NV\n",
    "            continue\n",
    "        # if label == \"NV\":\n",
    "        #     if num_samples_nevus >= 452:\n",
    "        #         continue\n",
    "        #     num_samples_nevus += 1\n",
    "        # if label == \"MEL\":\n",
    "        #     num_samples_mel += 1\n",
    "        image = cv2.imread(os.path.join(root, \"ISIC_2019_Training_Input\", img_name+\".jpg\"), cv2.IMREAD_COLOR)\n",
    "        image = cv2.resize(image, (224, 224), cv2.INTER_AREA) # remove later, inter area is for making it smaller, for making it larger use inter linear\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "        label = class_to_idx[isic_data_map[label]]\n",
    "        raw_data_test.append((image, label))\n",
    "\n",
    "    print(f\"Test set, Number of nevus samples: {num_samples_nevus}\\n Number of melanoma samples: {num_samples_mel}\")\n",
    "\n",
    "    return raw_data_train, raw_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_isic_2019()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# dataloader\n",
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.data[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "    \n",
    "# transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225] )\n",
    "])\n",
    "\n",
    "# datasets\n",
    "train_dataset = ISICDataset(train, transform=transform)\n",
    "test_dataset = ISICDataset(test, transform=transform)\n",
    "\n",
    "# dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(2048, 2)\n",
    "model.load_state_dict(torch.load(\"./saved_models/melanocytic_exp2.pth\"))\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "train_dataset_refined = []\n",
    "test_dataset_refined = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_dataset:\n",
    "        images = images.unsqueeze(0)\n",
    "        images = images.to(device)\n",
    "        labels = torch.Tensor([labels]).long()\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        if (labels == 1 and predicted == 0) or (labels == 0):\n",
    "            train_dataset_refined.append((images, labels.item()))\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Train Accuracy: {100 * correct / total}\")\n",
    "    print(f\"Size of refined train dataset: {len(train_dataset_refined)}\")\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_dataset:\n",
    "        images = images.unsqueeze(0)\n",
    "        images = images.to(device)\n",
    "        labels = torch.Tensor([labels]).long()\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        if (labels == 1 and predicted == 0) or (labels == 0):\n",
    "            test_dataset_refined.append((images, labels.item()))\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total}\")\n",
    "    print(f\"Size of refined test dataset: {len(test_dataset_refined)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_refined_fixed = []\n",
    "for images, labels in train_dataset_refined:\n",
    "    train_dataset_refined_fixed.append((images.squeeze(0), labels))\n",
    "\n",
    "test_dataset_refined_fixed = []\n",
    "for images, labels in test_dataset_refined:\n",
    "    test_dataset_refined_fixed.append((images.squeeze(0), labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset_refined_fixed, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset_refined_fixed, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet101()\n",
    "\n",
    "model.fc = nn.Linear(2048, 2)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    print('\\nTest set: Accuracy: {:.0f}%\\n'.format(\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "def train_model(model, train_loader, test_loader, optimizer, epochs):\n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss/100))\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        acc = test_model(model, test_loader)\n",
    "        acc_list.append(acc)\n",
    "        loss_list.append(running_loss)\n",
    "\n",
    "    return acc_list, loss_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list, loss_list = train_model(model, train_dataloader, test_dataloader, optimizer, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc_list)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss_list)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc_list)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.hlines(0.5, 0, 14, colors='r', linestyles='dashed', label='Baseline')\n",
    "plt.legend()\n",
    "plt.xlim(0, 15)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss_list)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_acc = test_model(model, test_dataloader)\n",
    "print(\"Final test accuracy: {:.0f}%\".format(100. * final_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classwise_acc(model, test_loader):\n",
    "    classes = (\"Melanoma\", \"Melanocytic Nevus\")\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "    # print accuracy for each class\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
    "    return [correct_pred[\"Melanoma\"] / total_pred[\"Melanoma\"], correct_pred[\"Melanocytic Nevus\"] / total_pred[\"Melanocytic Nevus\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classwise_score = classwise_acc(model, test_dataloader)\n",
    "print(\"Melanoma class accuracy: {:.0f}%\".format(100. * classwise_score[0]))\n",
    "print(\"Melanocytic Nevus class accuracy: {:.0f}%\".format(100. * classwise_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and standard deviation of accuracy \n",
    "mean = np.mean(acc_list)\n",
    "std = np.std(acc_list)\n",
    "print(\"Mean accuracy: {:.2f}%\".format(100. * mean))\n",
    "print(\"Standard deviation of accuracy: {:.2f}%\".format(100. * std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.module.state_dict(), \"./saved_models/dysplastic_nevus_exp1.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
